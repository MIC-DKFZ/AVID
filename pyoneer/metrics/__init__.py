# AVID - pyoneer
# AVID based tool for algorithmic evaluation and optimization
#
# Copyright (c) German Cancer Research Center,
# Software development for Integrated Diagnostic and Therapy (SIDT).
# All rights reserved.
#
# This software is distributed WITHOUT ANY WARRANTY; without
# even the implied warranty of MERCHANTABILITY or FITNESS FOR
# A PARTICULAR PURPOSE.
#
# See LICENSE.txt or http://www.dkfz.de/en/sidt/index.html for details.

from builtins import str
from builtins import object
import datetime
import logging
import os
import shutil
import subprocess

from avid.common import osChecker
from pyoneer.evaluationResult import EvaluationResult, MeasurementResult

from avid.common.artefact import defaultProps, ensureValidPath
from avid.common.artefact.fileHelper import loadArtefactList_xml
from avid.selectors import SelectorBase, ValiditySelector
from pyoneer.dataSetEvaluator import DataSetEvaluator

logger = logging.getLogger(__name__)

class DefaultMetric (object):
  '''Base class for metrics. A metric implements a method to
  evaluate an avid workflow according to result artefacts it produces. In
  order to evaluate, the metric uses the specified criteria.'''
  
  def __init__(self, metricCriteria, sessionDir, measureWeights = None, instanceDefiningProps = [defaultProps.CASE],
               instanceDefiningSelector = SelectorBase(), clearSessionDir = True, nrOfRetries = 0):
    '''Initialization of the metric.
    @param metricCriteria: list of criterion instances that should be used to
    evaluate.
    @param sessionDir: specifies the directory where the session for evaluation will be created. 
    @param instanceDefiningProps: List of artefact properties that discriminate
    evaluation instances. E.g. as a default, the case property will be used to
    discriminate evaluation instances, thus one instance evaluation per case.
    @param instanceDefiningSelector: Selector that is used to select the artefacts that are used to discriminate the
    evaluation instances. As default all artefacts of the evaluated workflow will be used.
    @param measureWeights: Weights that should be used when computing the single value
    measurement (by summing up the measurements of all criteria) or the weighted measurements.
    It is a dictionary with the measurement value IDs as keys and the weights as values. All
    measurement values not declared in the dict are assumed to have a weight of 0.
    Passing None implies that everything has a weight of 1.
    @param cleareSessionDir: Indicates of the metric should remove all artefacts
     generated by an evaluation or if they should be kept on disk.
    @param nrOfRetries: Number of retries, if the workflow results with invalid artefacts/failed actions. Default is 0, so
     no retries.
    ''' 
    self._metricCriteria = metricCriteria
    self._instanceDefiningProps = instanceDefiningProps
    self._instanceDefiningSelector = instanceDefiningSelector
    self.sessionDir = sessionDir
    self._measureWeights = measureWeights
    self._clearSessionDir = clearSessionDir
    self._nrOfRetries = nrOfRetries
    
    
  def evaluate(self, workflowFile, artefactFile, workflowModifier = None, label = None, silent = True):
    '''Function is called to evaluate a workflow used the passed artfact definitions
    @param workflowFile: String defining the path to the avid workflow that should
    be executed.
    @param artefactFile: String defining the path to the artefact bootstrap file
     for the workflow session that will be evaluated.
    @param workflowModifier: Dict that defines the modifier for the workflow.
    They will be passed as arguments to the workflow execution. Key is the argument
    name and the dict value the argument value. The default implementation will
    generate an cl argument signature like --<key> <value>. Thus {'a':120} will
    be "--a 120". To change this behavior, override DefaultMetric._generateWorkflowCall(...)
    @param label: You can specify a label that is used by the metric when defining
    the workflow session path. If not specified a unique label will be generated.
    @param silent: Indicates if the workflow should be executed silently (no direct stdout) or not.
    @return: Returns a EvaluationResult instance with everything you want to know.
    '''
    sessionFile,sessionName = self._generateSessionPath(label)

    global logger

    useShell = not osChecker.isWindows()

    artefacts = None

    for runCount in range(self._nrOfRetries+1):
      if runCount>0:
        logger.info('Failed actions occured. Retry (#{}) workflow...'.format(runCount))

      callStr = self._generateWorkflowCall(workflowFile, sessionFile, sessionName, artefactFile, workflowModifier,
                                           overWriteExistingSession=(runCount==0))
      logger.debug(
        'Evaluate workflow. workflow file: "%s"; artefact file: "%s"; workflow modifier: "%s"; session: "%s"',
        workflowFile, artefactFile, workflowModifier, sessionFile)
      logger.debug('Starting workflow processing... Call: "%s"', callStr)

      if silent:
        subprocess.call(callStr, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=useShell)
      else:
        subprocess.call(callStr, shell=useShell)

      try:
        artefacts = loadArtefactList_xml(sessionFile, True, self.sessionDir)
      except:
        logger.debug('Error. Cannot load artefacts. Evaluation candidate workflow might have failed.')

      failedArtefacts = ValiditySelector(negate=True).getSelection(artefacts)

      if len(failedArtefacts)>0:
        logger.debug('Some artefacts are invalid, indicating failed actions. Failed artefacts: {}'.format(failedArtefacts))
      else:
        break



    logger.debug('Evaluating workflow results...')
    
    evaluator = DataSetEvaluator(self._metricCriteria, self._instanceDefiningProps, self._instanceDefiningSelector)
    


    gmeasure = dict()
    imeasure = dict()
    try:
      artefacts = loadArtefactList_xml(sessionFile, True, self.sessionDir)
      gmeasure, imeasure = evaluator.evaluate(artefacts)
    except:
      logger.debug('Error. Cannot load artefacts. Evaluation candidate workflow might have failed.')
    

       
    if self._clearSessionDir:
      artefactDir = os.path.join(self.sessionDir, sessionName)
      logger.debug('Remove session directory after evaluation. Dir: "%s"', artefactDir)
      
      try:
        shutil.rmtree(artefactDir)
        os.remove(sessionFile)
        os.remove(sessionFile+os.extsep+'log')
      except:
        logger.debug('Unkown error when clearing the session dir.')       

    result = EvaluationResult(measurements=gmeasure, instanceMeasurements=imeasure, workflowModifier=workflowModifier,
                              measureWeights=self._measureWeights, name=sessionName, workflowFile=workflowFile,
                              artefactFile=artefactFile, valueNames=self.valueNames, valueDescriptions=self.valueDescriptions)
      
    return result


  def _generateWorkflowCall(self, workflowFile, sessionFile, sessionName, artefactFile, workflowModifier = None,
                            overWriteExistingSession = True):
    '''Helper function generating the cl call that runs the workflow
    @param workflowFile: String defining the path to the avid workflow that should
    be executed.
    @param sessionFile: Path where the session of the workflow call should be stored.
    @param sessionName: Name of the session of the workflow call.
    @param artefactFile: String defining the path to the artefact bootstrap file
     for the workflow session that will be evaluated.
    @param workflowModifier: Dict that defines the modifier for the workflow.
    They will be passed as arguments to the workflow execution. Key is the argument
    name and the dict value the argument value. The default implementation will
    generate an cl argument signature like --<key> <value>. Thus {'a':120} will
    be "--a 120". To change this behavior, override MetricBase._generateWorkflowCall(...)
    @return: Returns a EvaluationResult instance with everythin you want to know.
    '''
    
    callStr = 'python "'+workflowFile\
      +'" --sessionPath "'+sessionFile\
      +'" --name "'+sessionName
    if artefactFile is not None:
      callStr += '" --bootstrapArtefacts "'+artefactFile

    callStr += '" --autoSave --debug'

    if overWriteExistingSession:
      callStr += ' --overwriteExistingSession'
      
    if workflowModifier is not None:
      for modKey in workflowModifier:
        callStr = callStr+' --'+str(modKey)+' "'+str(workflowModifier[modKey])+'"'
      
    return callStr
    
    
  def _generateSessionPath(self, label = None):
    '''Helper function that returns a tuple (<path to the session file>, <session name>)'''
    name = label
    if label is None:
      name = datetime.datetime.now().strftime("EvalSession_%Y-%m-%d_%H-%M-%S")
    
    return (ensureValidPath(os.path.join(self.sessionDir, name+os.extsep+"avid")), name)

  @property
  def valueNames(self):
    '''Returns a dict with all valueID:valueNames of measurements the metric (and
    it criteria) generates.'''
    
    result = dict()

    for criterion in self._metricCriteria:
      result.update(criterion.valueNames)
    
    return result

  @property
  def valueDescriptions(self):
    '''Returns a dict with all valueID:valueDescriptions of measurements the metric (and
    it criteria) generates.'''
    
    result = dict()

    for criterion in self._metricCriteria:
      result.update(criterion.valueDescriptions)
    
    return result
    
